{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81753e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We first import the libraries\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9b631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data with normalization\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649f544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=243, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=243, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Building the neural network model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(28 * 28, 1024) #First layer\n",
    "        self.dropout1 = nn.Dropout(p = 0.2) #First layer dropout\n",
    "        self.fc2 = nn.Linear(1024, 243) #Second layer\n",
    "        self.dropout2 = nn.Dropout(p = 0.5)#Second layer dropout\n",
    "        self.fc3 = nn.Linear(243, 10) #Third layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "#Check the model\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152d7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "#Load the optimizer and the criterion\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.003)\n",
    "losses_train = []\n",
    "losses_valid = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08808d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing early stoping to maximize the time efficiency and avoid overfitting\n",
    "patience = 15\n",
    "min_delta = 0.001\n",
    "best_loss = None\n",
    "patience_counter = 0\n",
    "num_epochs=500\n",
    "\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_set, [50000, 10000], generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(val_subset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f9c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.799235422855247, Validation Loss: 1.0717783817060433\n",
      "Epoch 2, Training Loss: 0.8652400755996643, Validation Loss: 0.5698935626798375\n",
      "Epoch 3, Training Loss: 0.5981584552254504, Validation Loss: 0.4372434380707467\n",
      "Epoch 4, Training Loss: 0.5016528842196282, Validation Loss: 0.3829126436809066\n",
      "Epoch 5, Training Loss: 0.4513381414576126, Validation Loss: 0.34860745599125603\n",
      "Epoch 6, Training Loss: 0.4161464842016509, Validation Loss: 0.3259182135295716\n",
      "Epoch 7, Training Loss: 0.3899782095223602, Validation Loss: 0.30937020931464093\n",
      "Epoch 8, Training Loss: 0.37150511819162346, Validation Loss: 0.29629017972642446\n",
      "Epoch 9, Training Loss: 0.35500958357760903, Validation Loss: 0.27990970323989345\n",
      "Epoch 10, Training Loss: 0.3380413874745496, Validation Loss: 0.27034021937733244\n",
      "Epoch 11, Training Loss: 0.3267412964643827, Validation Loss: 0.25915931165218353\n",
      "Epoch 12, Training Loss: 0.3145349177438567, Validation Loss: 0.24919472338192783\n",
      "Epoch 13, Training Loss: 0.30080607639097456, Validation Loss: 0.24028139589888275\n",
      "Epoch 14, Training Loss: 0.2933193192616709, Validation Loss: 0.23268211694659702\n",
      "Epoch 15, Training Loss: 0.2832501288860846, Validation Loss: 0.22408865370852932\n",
      "Epoch 16, Training Loss: 0.27486848325204494, Validation Loss: 0.21601725260543217\n",
      "Epoch 17, Training Loss: 0.26228111121319, Validation Loss: 0.2090434206614069\n",
      "Epoch 18, Training Loss: 0.2559864547000384, Validation Loss: 0.20267564195926022\n",
      "Epoch 19, Training Loss: 0.24619855903295565, Validation Loss: 0.19529185593602763\n",
      "Epoch 20, Training Loss: 0.24142409946475582, Validation Loss: 0.1891933340745367\n",
      "Epoch 21, Training Loss: 0.2339184259904473, Validation Loss: 0.18360667504892228\n",
      "Epoch 22, Training Loss: 0.22771229254189077, Validation Loss: 0.17885073020484796\n",
      "Epoch 23, Training Loss: 0.22102556879650045, Validation Loss: 0.17254168931750735\n",
      "Epoch 24, Training Loss: 0.21511953092539615, Validation Loss: 0.16728848361285628\n",
      "Epoch 25, Training Loss: 0.20819845147478555, Validation Loss: 0.16201025304520966\n",
      "Epoch 26, Training Loss: 0.2020513045349355, Validation Loss: 0.1575271146027905\n",
      "Epoch 27, Training Loss: 0.20010061332507173, Validation Loss: 0.15322619650489205\n",
      "Epoch 28, Training Loss: 0.19160135765510328, Validation Loss: 0.14905358382328682\n",
      "Epoch 29, Training Loss: 0.1895287763287645, Validation Loss: 0.14542555142265218\n",
      "Epoch 30, Training Loss: 0.18415662663371196, Validation Loss: 0.1409574397809946\n",
      "Epoch 31, Training Loss: 0.18215878828089122, Validation Loss: 0.1373848681735575\n",
      "Epoch 32, Training Loss: 0.17602887493905736, Validation Loss: 0.13435790287983265\n",
      "Epoch 33, Training Loss: 0.1714835089009835, Validation Loss: 0.1309006789544965\n",
      "Epoch 34, Training Loss: 0.1679851662780621, Validation Loss: 0.12729209713685286\n",
      "Epoch 35, Training Loss: 0.16434792320388975, Validation Loss: 0.1247099226660979\n",
      "Epoch 36, Training Loss: 0.1626014217182295, Validation Loss: 0.12163283260669677\n",
      "Epoch 37, Training Loss: 0.15825134815612454, Validation Loss: 0.11927197567501645\n",
      "Epoch 38, Training Loss: 0.15388471838920864, Validation Loss: 0.11615699247285059\n",
      "Epoch 39, Training Loss: 0.15170219706208593, Validation Loss: 0.11386610979249903\n",
      "Epoch 40, Training Loss: 0.14823665987771711, Validation Loss: 0.11122208620142784\n",
      "Epoch 41, Training Loss: 0.14632278039598706, Validation Loss: 0.1094206233455497\n",
      "Epoch 42, Training Loss: 0.14402028557850416, Validation Loss: 0.1061322391389092\n",
      "Epoch 43, Training Loss: 0.14148847710714538, Validation Loss: 0.10524941887706518\n",
      "Epoch 44, Training Loss: 0.13839082567811584, Validation Loss: 0.10185192514116027\n",
      "Epoch 45, Training Loss: 0.13292391358145964, Validation Loss: 0.09991232844389927\n",
      "Epoch 46, Training Loss: 0.13370621898003035, Validation Loss: 0.09820971465936512\n",
      "Epoch 47, Training Loss: 0.13114507385551422, Validation Loss: 0.09707481229594749\n",
      "Epoch 48, Training Loss: 0.12727576328405757, Validation Loss: 0.09450731610369151\n",
      "Epoch 49, Training Loss: 0.12515054777788837, Validation Loss: 0.09184288294024907\n",
      "Epoch 50, Training Loss: 0.12345060903920548, Validation Loss: 0.0897826267062289\n",
      "Epoch 51, Training Loss: 0.12232007508251522, Validation Loss: 0.08934655329983705\n",
      "Epoch 52, Training Loss: 0.12146275570350033, Validation Loss: 0.08666706137406598\n",
      "Epoch 53, Training Loss: 0.11623457185963769, Validation Loss: 0.08543828168919512\n",
      "Epoch 54, Training Loss: 0.11670361207857696, Validation Loss: 0.08337320321518335\n",
      "Epoch 55, Training Loss: 0.11408438494325733, Validation Loss: 0.08226735357812066\n",
      "Epoch 56, Training Loss: 0.11277562945779325, Validation Loss: 0.08158308373182822\n",
      "Epoch 57, Training Loss: 0.11185992797558654, Validation Loss: 0.0794987642639287\n",
      "Epoch 58, Training Loss: 0.10839156057836531, Validation Loss: 0.07788660779749607\n",
      "Epoch 59, Training Loss: 0.10711953403956409, Validation Loss: 0.07669383145062028\n",
      "Epoch 60, Training Loss: 0.10490785542128882, Validation Loss: 0.07505089077790073\n",
      "Epoch 61, Training Loss: 0.10469487540797194, Validation Loss: 0.07397885425431523\n",
      "Epoch 62, Training Loss: 0.10279741490990527, Validation Loss: 0.07243191484943222\n",
      "Epoch 63, Training Loss: 0.10025126446904277, Validation Loss: 0.07161883547142812\n",
      "Epoch 64, Training Loss: 0.0994497091332669, Validation Loss: 0.0701315030603889\n",
      "Epoch 65, Training Loss: 0.09780619993134698, Validation Loss: 0.06913673529211598\n",
      "Epoch 66, Training Loss: 0.09758177642096906, Validation Loss: 0.06796398097174658\n",
      "Epoch 67, Training Loss: 0.09581976178795226, Validation Loss: 0.06657494776911892\n",
      "Epoch 68, Training Loss: 0.0964319659238145, Validation Loss: 0.06795063979069518\n",
      "Epoch 69, Training Loss: 0.09254237159943657, Validation Loss: 0.06459589447577004\n",
      "Epoch 70, Training Loss: 0.0931651286874165, Validation Loss: 0.0636489832548389\n",
      "Epoch 71, Training Loss: 0.09217208303681919, Validation Loss: 0.06302357445156594\n",
      "Epoch 72, Training Loss: 0.09031136948039442, Validation Loss: 0.061824268584323536\n",
      "Epoch 73, Training Loss: 0.08777557504373287, Validation Loss: 0.06096617871551377\n",
      "Epoch 74, Training Loss: 0.08673462704983728, Validation Loss: 0.05949450541394436\n",
      "Epoch 75, Training Loss: 0.08676371652025308, Validation Loss: 0.058705840746213676\n",
      "Epoch 76, Training Loss: 0.08532407505215327, Validation Loss: 0.058313392233222155\n",
      "Epoch 77, Training Loss: 0.0837616998432224, Validation Loss: 0.05701343917471778\n",
      "Epoch 78, Training Loss: 0.08278499176002928, Validation Loss: 0.05606175607343199\n",
      "Epoch 79, Training Loss: 0.08222370648454787, Validation Loss: 0.05548077061250331\n",
      "Epoch 80, Training Loss: 0.07997256320919087, Validation Loss: 0.05441990597054932\n",
      "Epoch 81, Training Loss: 0.07937154013798563, Validation Loss: 0.05486251610764273\n",
      "Epoch 82, Training Loss: 0.07941854515694764, Validation Loss: 0.05337473310850513\n",
      "Epoch 83, Training Loss: 0.07798001784950431, Validation Loss: 0.05253012206666409\n",
      "Epoch 84, Training Loss: 0.078137839398186, Validation Loss: 0.05136828304262488\n",
      "Epoch 85, Training Loss: 0.07626188427471975, Validation Loss: 0.05158374363639552\n",
      "Epoch 86, Training Loss: 0.07553770773009515, Validation Loss: 0.049562086274099956\n",
      "Epoch 87, Training Loss: 0.07445870710513008, Validation Loss: 0.05017915645400715\n",
      "Epoch 88, Training Loss: 0.07446258593160016, Validation Loss: 0.04873638734470602\n",
      "Epoch 89, Training Loss: 0.07199750479801234, Validation Loss: 0.04757392489808095\n",
      "Epoch 90, Training Loss: 0.07214946577103058, Validation Loss: 0.04695815279498507\n",
      "Epoch 91, Training Loss: 0.0714521830681084, Validation Loss: 0.04706671654994178\n",
      "Epoch 92, Training Loss: 0.07045671734055366, Validation Loss: 0.0455736949769958\n",
      "Epoch 93, Training Loss: 0.06817188852333597, Validation Loss: 0.045326954335164114\n",
      "Epoch 94, Training Loss: 0.06750755966243856, Validation Loss: 0.04517026848284302\n",
      "Epoch 95, Training Loss: 0.06918786985597122, Validation Loss: 0.04402988661632274\n",
      "Epoch 96, Training Loss: 0.06767027805557352, Validation Loss: 0.04433715706513186\n",
      "Epoch 97, Training Loss: 0.06729925832729032, Validation Loss: 0.04277185765302675\n",
      "Epoch 98, Training Loss: 0.06474563911426137, Validation Loss: 0.04166953890157306\n",
      "Epoch 99, Training Loss: 0.06495781950126372, Validation Loss: 0.04131827006786824\n",
      "Epoch 100, Training Loss: 0.06459362658687921, Validation Loss: 0.04096303386228147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, Training Loss: 0.06292875997026139, Validation Loss: 0.04025447270299077\n",
      "Epoch 102, Training Loss: 0.0623419243134439, Validation Loss: 0.03969167040180486\n",
      "Epoch 103, Training Loss: 0.06281344621910898, Validation Loss: 0.03919575484558513\n",
      "Epoch 104, Training Loss: 0.061095559642886495, Validation Loss: 0.03969890774660714\n",
      "Epoch 105, Training Loss: 0.06066786612310548, Validation Loss: 0.03860261392405933\n",
      "Epoch 106, Training Loss: 0.0609412642060789, Validation Loss: 0.03747358871475574\n",
      "Epoch 107, Training Loss: 0.060170630595759986, Validation Loss: 0.03766214546219558\n",
      "Epoch 108, Training Loss: 0.058557060133638, Validation Loss: 0.036587611307941706\n",
      "Epoch 109, Training Loss: 0.05840781094011928, Validation Loss: 0.03584487132051853\n",
      "Epoch 110, Training Loss: 0.059105219766755764, Validation Loss: 0.03563123325829151\n",
      "Epoch 111, Training Loss: 0.05822298278225892, Validation Loss: 0.035275983745766105\n",
      "Epoch 112, Training Loss: 0.05714541757012656, Validation Loss: 0.034695890236788304\n",
      "Epoch 113, Training Loss: 0.056268712823916635, Validation Loss: 0.034298029373797355\n",
      "Epoch 114, Training Loss: 0.05699537052010407, Validation Loss: 0.03428445375749877\n",
      "Epoch 115, Training Loss: 0.05636564508201614, Validation Loss: 0.03309957017220414\n",
      "Epoch 116, Training Loss: 0.054799433040525966, Validation Loss: 0.033949452978825775\n",
      "Epoch 117, Training Loss: 0.05528197114639826, Validation Loss: 0.032317824207934415\n",
      "Epoch 118, Training Loss: 0.053399174484604996, Validation Loss: 0.031627976388973036\n",
      "Epoch 119, Training Loss: 0.052875516957033485, Validation Loss: 0.03186338866251123\n",
      "Epoch 120, Training Loss: 0.05361008090101111, Validation Loss: 0.03151385180628414\n",
      "Epoch 121, Training Loss: 0.05185938922952471, Validation Loss: 0.030542831503115833\n",
      "Epoch 122, Training Loss: 0.052631363228980155, Validation Loss: 0.030513935307171315\n",
      "Epoch 123, Training Loss: 0.05039744114211357, Validation Loss: 0.030472787803003363\n",
      "Epoch 124, Training Loss: 0.04938391776981829, Validation Loss: 0.029231670757482765\n",
      "Epoch 125, Training Loss: 0.05059925239728784, Validation Loss: 0.02939860476329211\n",
      "Epoch 126, Training Loss: 0.04930646318509412, Validation Loss: 0.028509678970132568\n",
      "Epoch 127, Training Loss: 0.04866959019623665, Validation Loss: 0.028136108786158357\n",
      "Epoch 128, Training Loss: 0.047670023108739204, Validation Loss: 0.028100388834941065\n",
      "Epoch 129, Training Loss: 0.04846695361923037, Validation Loss: 0.027678350508687603\n",
      "Epoch 130, Training Loss: 0.04810698754759803, Validation Loss: 0.027596484397383776\n",
      "Epoch 131, Training Loss: 0.047394894658705035, Validation Loss: 0.026634464334773646\n",
      "Epoch 132, Training Loss: 0.046481624466807904, Validation Loss: 0.027038389197728673\n",
      "Epoch 133, Training Loss: 0.04723141750028885, Validation Loss: 0.02642440953689396\n",
      "Epoch 134, Training Loss: 0.04715552173261005, Validation Loss: 0.026120933299961315\n",
      "Epoch 135, Training Loss: 0.04531311305629006, Validation Loss: 0.026060877705977602\n",
      "Epoch 136, Training Loss: 0.04624939054798788, Validation Loss: 0.02518826542173032\n",
      "Epoch 137, Training Loss: 0.0448951681893168, Validation Loss: 0.024692716171140456\n",
      "Epoch 138, Training Loss: 0.0441591058853271, Validation Loss: 0.024862098549848576\n",
      "Epoch 139, Training Loss: 0.04430193622395007, Validation Loss: 0.024292194446145443\n",
      "Epoch 140, Training Loss: 0.04392771685976527, Validation Loss: 0.023740227099146197\n",
      "Epoch 141, Training Loss: 0.042540706517145054, Validation Loss: 0.023826103320843568\n",
      "Epoch 142, Training Loss: 0.04342922256407596, Validation Loss: 0.02317710516301049\n",
      "Epoch 143, Training Loss: 0.04282712939280723, Validation Loss: 0.02300372230672058\n",
      "Epoch 144, Training Loss: 0.042830529328344316, Validation Loss: 0.022578295226843587\n",
      "Epoch 145, Training Loss: 0.041011424055809514, Validation Loss: 0.022341932971653333\n",
      "Epoch 146, Training Loss: 0.041512922999431205, Validation Loss: 0.022223044034081756\n",
      "Epoch 147, Training Loss: 0.04156749136919684, Validation Loss: 0.021680787964249444\n",
      "Epoch 148, Training Loss: 0.041248390920210015, Validation Loss: 0.021752290777672247\n",
      "Epoch 149, Training Loss: 0.04035133333516313, Validation Loss: 0.021705289894068364\n",
      "Epoch 150, Training Loss: 0.04008473381737688, Validation Loss: 0.02112496551496991\n",
      "Epoch 151, Training Loss: 0.0396090507231055, Validation Loss: 0.02051501333488116\n",
      "Epoch 152, Training Loss: 0.03821399140192557, Validation Loss: 0.020764869176575048\n",
      "Epoch 153, Training Loss: 0.03836425019131263, Validation Loss: 0.02088168693875455\n",
      "Epoch 154, Training Loss: 0.03869076414745666, Validation Loss: 0.019838321148744148\n",
      "Epoch 155, Training Loss: 0.03864517495749014, Validation Loss: 0.019826828570336484\n",
      "Epoch 156, Training Loss: 0.038188561367720905, Validation Loss: 0.019562452471708274\n",
      "Epoch 157, Training Loss: 0.037341426619376214, Validation Loss: 0.019522781175365492\n",
      "Epoch 158, Training Loss: 0.03737627703193595, Validation Loss: 0.019409016163272272\n",
      "Epoch 159, Training Loss: 0.03788029414111141, Validation Loss: 0.018709420385195095\n",
      "Epoch 160, Training Loss: 0.03694488075119989, Validation Loss: 0.018481364378788667\n",
      "Epoch 161, Training Loss: 0.03645598368474971, Validation Loss: 0.01837836725379275\n",
      "Epoch 162, Training Loss: 0.03662042634840657, Validation Loss: 0.017983433723028536\n",
      "Epoch 163, Training Loss: 0.03620031787438004, Validation Loss: 0.017588852556974028\n",
      "Epoch 164, Training Loss: 0.03534205575837994, Validation Loss: 0.017594766881684685\n",
      "Epoch 165, Training Loss: 0.03518521589468291, Validation Loss: 0.01716495432096065\n",
      "Epoch 166, Training Loss: 0.034792156329801455, Validation Loss: 0.016984545422204813\n",
      "Epoch 167, Training Loss: 0.03412579552720744, Validation Loss: 0.016684557809894845\n",
      "Epoch 168, Training Loss: 0.03521079100620772, Validation Loss: 0.016623737230440187\n",
      "Epoch 169, Training Loss: 0.03377165392238293, Validation Loss: 0.016327290186972897\n",
      "Epoch 170, Training Loss: 0.03473270350525072, Validation Loss: 0.016873193771287705\n",
      "Epoch 171, Training Loss: 0.033635514145935456, Validation Loss: 0.016198896360287002\n",
      "Epoch 172, Training Loss: 0.03314673218196992, Validation Loss: 0.015931819031715012\n",
      "Epoch 173, Training Loss: 0.03346246886062128, Validation Loss: 0.016018134551449043\n",
      "Epoch 174, Training Loss: 0.03148838928712012, Validation Loss: 0.016083825357735844\n",
      "Epoch 175, Training Loss: 0.03248643750403779, Validation Loss: 0.01534560557742526\n",
      "Epoch 176, Training Loss: 0.03260698141743626, Validation Loss: 0.014759832167120021\n",
      "Epoch 177, Training Loss: 0.03170541568256136, Validation Loss: 0.015053808046608665\n",
      "Epoch 178, Training Loss: 0.031094320446326297, Validation Loss: 0.015239511959842009\n",
      "Epoch 179, Training Loss: 0.031132781340840902, Validation Loss: 0.014690740829983217\n",
      "Epoch 180, Training Loss: 0.03197180275442396, Validation Loss: 0.014569318234167138\n",
      "Epoch 181, Training Loss: 0.030924055503178904, Validation Loss: 0.014448135158426727\n",
      "Epoch 182, Training Loss: 0.03116823307825312, Validation Loss: 0.014220435334336914\n",
      "Epoch 183, Training Loss: 0.030179267087310894, Validation Loss: 0.013648574125202621\n",
      "Epoch 184, Training Loss: 0.030240847854249513, Validation Loss: 0.013582528439412713\n",
      "Epoch 185, Training Loss: 0.030490981908748424, Validation Loss: 0.013474636870973215\n",
      "Epoch 186, Training Loss: 0.029896490851234494, Validation Loss: 0.013600177977643433\n",
      "Epoch 187, Training Loss: 0.02957724143903869, Validation Loss: 0.013317033460520943\n",
      "Epoch 188, Training Loss: 0.02897628113976928, Validation Loss: 0.013393175397083092\n",
      "Epoch 189, Training Loss: 0.028487671865150332, Validation Loss: 0.013140021654655623\n",
      "Epoch 190, Training Loss: 0.029020697369537097, Validation Loss: 0.012793923136451916\n",
      "Epoch 191, Training Loss: 0.028196794111460352, Validation Loss: 0.012824972777087598\n",
      "Epoch 192, Training Loss: 0.02833176330317741, Validation Loss: 0.012286285915008634\n",
      "Epoch 193, Training Loss: 0.028098127318060658, Validation Loss: 0.011874619983715024\n",
      "Epoch 194, Training Loss: 0.029031081134076122, Validation Loss: 0.01185745427831355\n",
      "Epoch 195, Training Loss: 0.02752900430868104, Validation Loss: 0.011996909363898241\n",
      "Epoch 196, Training Loss: 0.02811212523647749, Validation Loss: 0.011591655640806529\n",
      "Epoch 197, Training Loss: 0.028235551104126278, Validation Loss: 0.011621476138443656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198, Training Loss: 0.027504187731964908, Validation Loss: 0.011799652680345354\n",
      "Epoch 199, Training Loss: 0.027395609491961456, Validation Loss: 0.01151845128361468\n",
      "Epoch 200, Training Loss: 0.026920308900335782, Validation Loss: 0.011389835220820916\n",
      "Epoch 201, Training Loss: 0.026722530116151368, Validation Loss: 0.011236133572206519\n",
      "Epoch 202, Training Loss: 0.027033077650465198, Validation Loss: 0.011188703280677842\n",
      "Epoch 203, Training Loss: 0.027058026622926223, Validation Loss: 0.010790815215059526\n",
      "Epoch 204, Training Loss: 0.025957184249727823, Validation Loss: 0.010692670675525501\n",
      "Epoch 205, Training Loss: 0.025677350141839788, Validation Loss: 0.010736150971827375\n",
      "Epoch 206, Training Loss: 0.025859253885629558, Validation Loss: 0.010669829679065782\n",
      "Epoch 207, Training Loss: 0.024478376433844054, Validation Loss: 0.01078501620361801\n",
      "Epoch 208, Training Loss: 0.02487416560218028, Validation Loss: 0.0103408716775471\n",
      "Epoch 209, Training Loss: 0.02419163696671417, Validation Loss: 0.009955509373537045\n",
      "Epoch 210, Training Loss: 0.024440455024685663, Validation Loss: 0.009781479109953615\n",
      "Epoch 211, Training Loss: 0.024861802391435848, Validation Loss: 0.010141632466645211\n",
      "Epoch 212, Training Loss: 0.02535765810059443, Validation Loss: 0.010082103426577109\n",
      "Epoch 213, Training Loss: 0.024683384248601604, Validation Loss: 0.01022708092172553\n",
      "Epoch 214, Training Loss: 0.024811424772197537, Validation Loss: 0.009840588972556767\n",
      "Epoch 215, Training Loss: 0.023652800052155698, Validation Loss: 0.009500708248040633\n",
      "Epoch 216, Training Loss: 0.023324060271050273, Validation Loss: 0.009224621029229609\n",
      "Epoch 217, Training Loss: 0.022924352281251306, Validation Loss: 0.009650743256136182\n",
      "Epoch 218, Training Loss: 0.023194452033572788, Validation Loss: 0.008886617223901236\n",
      "Epoch 219, Training Loss: 0.023339293693611297, Validation Loss: 0.009142606197537522\n",
      "Epoch 220, Training Loss: 0.023492060275177466, Validation Loss: 0.009403095127581649\n",
      "Epoch 221, Training Loss: 0.02310498898487979, Validation Loss: 0.008840640508725493\n",
      "Epoch 222, Training Loss: 0.02289272995585246, Validation Loss: 0.008571283807636944\n",
      "Epoch 223, Training Loss: 0.023849288472406336, Validation Loss: 0.008689760442735674\n",
      "Epoch 224, Training Loss: 0.022002004094542, Validation Loss: 0.008693316169483527\n",
      "Epoch 225, Training Loss: 0.021881021980095956, Validation Loss: 0.008265579015144093\n",
      "Epoch 226, Training Loss: 0.022114517116077197, Validation Loss: 0.008428135924419435\n",
      "Epoch 227, Training Loss: 0.021890603769784057, Validation Loss: 0.008340916968214734\n",
      "Epoch 228, Training Loss: 0.022267948260465298, Validation Loss: 0.008275716112678941\n",
      "Epoch 229, Training Loss: 0.021700272414729552, Validation Loss: 0.007997146527509793\n",
      "Epoch 230, Training Loss: 0.021775316729246062, Validation Loss: 0.007698615556167569\n",
      "Epoch 231, Training Loss: 0.021395232042432752, Validation Loss: 0.007693810062927853\n",
      "Epoch 232, Training Loss: 0.02138430657568199, Validation Loss: 0.007948700713244368\n",
      "Epoch 233, Training Loss: 0.020827523802644386, Validation Loss: 0.00765265051051559\n",
      "Epoch 234, Training Loss: 0.02116303593401851, Validation Loss: 0.007492991514128757\n",
      "Epoch 235, Training Loss: 0.020399605174973877, Validation Loss: 0.0073692002804552435\n",
      "Epoch 236, Training Loss: 0.020567984208896526, Validation Loss: 0.007628966713469523\n",
      "Epoch 237, Training Loss: 0.02071185497582447, Validation Loss: 0.007239570444120557\n",
      "Epoch 238, Training Loss: 0.021105051142543848, Validation Loss: 0.007523638592249933\n",
      "Epoch 239, Training Loss: 0.02056506715350296, Validation Loss: 0.00718203631702775\n",
      "Epoch 240, Training Loss: 0.020712617830907516, Validation Loss: 0.0071784514577163585\n",
      "Epoch 241, Training Loss: 0.020895098664188212, Validation Loss: 0.006827140443107101\n",
      "Epoch 242, Training Loss: 0.0205928101611417, Validation Loss: 0.007163833325527087\n",
      "Epoch 243, Training Loss: 0.019615349145720343, Validation Loss: 0.007147748774224537\n",
      "Epoch 244, Training Loss: 0.019496400955630575, Validation Loss: 0.007036489602119015\n",
      "Epoch 245, Training Loss: 0.019410856759862236, Validation Loss: 0.006964495173094877\n",
      "Early stopping triggered! 0.007698615556167569\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    # evaluation phase\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    # Calculate average losses\n",
    "    training_loss = running_loss / len(train_loader)\n",
    "    losses_train.append(training_loss)\n",
    "    validation_loss /= len(validation_loader)\n",
    "    losses_valid.append(validation_loss)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {training_loss}, Validation Loss: {validation_loss}\")\n",
    "\n",
    "    # Early stopping condition\n",
    "    if best_loss is None or validation_loss < best_loss - min_delta:\n",
    "        best_loss = validation_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\", best_loss)\n",
    "            break\n",
    "\n",
    "print(\"Training is finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "891e79d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is : 98.21%\n"
     ]
    }
   ],
   "source": [
    "#Accuracy counter\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "  for i in range(10000):\n",
    "    yval = model.forward(test_set[i][0])\n",
    "    if yval.argmax().item() == test_set[i][1] :\n",
    "      correct +=1\n",
    "\n",
    "\n",
    "print(f\"The accuracy of the model is : {100*correct/10000}%\") \n",
    "\n",
    "#Here we got an accuracy of 98.21% which is pretty decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e80afbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x258ababddf0>,\n",
       " <matplotlib.lines.Line2D at 0x258ababdeb0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjp0lEQVR4nO3deZgcd33n8fe3qo+5dM9I1mlJRmBkfCAGYWLwQRYjOziGxbtrh0DCAlry4JxLss6T5XjCskdINgmB4DiOMTmwNlkwaIMdG0LAYMdYI2PZso1sWZbt0cjW6BiNNFdf3/2jakatmW5NW+pRS9Wf1/P0U91Vv+r+lUf+/Kp+VfUrc3dERCT5gkZXQERETg8FvohIk1Dgi4g0CQW+iEiTUOCLiDSJVKMrUElnZ6evXLmy0dUQETlrbN26db+7d52ozBkZ+CtXrqSnp6fR1RAROWuY2QvTlVGXjohIk1Dgi4g0CQW+iEiTUOCLiDQJBb6ISJNQ4IuINAkFvohIk0hU4H/hn5/lB8/0N7oaIiJnpEQF/pe//xw/elaBLyJSSaICPwyMYqnRtRAROTMlKvADg5Ke4CUiUtG0Y+mY2R3Au4F97v6GCst/G3h/2fe9Huhy94Nmths4AhSBgrt316vilUR7+Ap8EZFKatnDvxPYUG2hu3/e3S9x90uA3wV+4O4Hy4pcFS+f0bAHCIOAovbwRUQqmjbw3f0B4OB05WI3AXedUo1OQRhAsajAFxGppG59+GbWRnQk8PWy2Q7cb2ZbzWzjNOtvNLMeM+vp7z+5K21CM+3hi4hUUc+TttcBD07qzrnM3dcB1wAfN7PLq63s7re5e7e7d3d1nXAM/6qCwCipD19EpKJ6Bv6NTOrOcfe+eLoPuBtYX8ffmyIMtIcvIlJNXQLfzOYAVwDfKpvXbmazxt8DVwPb6/F71YSmq3RERKqp5bLMu4ArgU4z6wU+DaQB3P3WuNh7gfvdfahs1UXA3WY2/jtfc/d/ql/VpwoC03X4IiJVTBv47n5TDWXuJLp8s3zeLuDik63YydAevohIdcm601ZDK4iIVJWowA8DDa0gIlJNsgJfXToiIlUlK/A1lo6ISFUKfBGRJpGowA80tIKISFWJCvxQQyuIiFSVuMDXHr6ISGWJCvzAtIcvIlJNogJfe/giItUlKvAD0522IiLVJCrwwwB16YiIVJGwwFeXjohINYkKfJ20FRGpLlGBnwqMggJfRKSiRAV+oKEVRESqSlTgh6YnXomIVJOswNcevohIVdMGvpndYWb7zKziA8jN7EozO2xmj8WvT5Ut22BmO8xsp5ndUs+KV6Jn2oqIVFfLHv6dwIZpyvzQ3S+JX78PYGYh8CXgGmAtcJOZrT2Vyk5HD0AREalu2sB39weAgyfx3euBne6+y91zwCbg+pP4npqpS0dEpLp69eG/1cy2mdm9ZnZBPG8p8FJZmd54XkVmttHMesysp7+//6QqEZihvBcRqawegf8ocK67Xwz8GfDNeL5VKFs1jt39Nnfvdvfurq6uk6pIGKA9fBGRKk458N190N2Pxu/vAdJm1km0R7+8rOgyoO9Uf+9EAg2tICJS1SkHvpmdY2YWv18ff+cBYAuwxsxWmVkGuBHYfKq/dyKhhlYQEakqNV0BM7sLuBLoNLNe4NNAGsDdbwVuAH7FzArACHCjuztQMLObgfuAELjD3Z+cka2IaWgFEZHqpg18d79pmuVfBL5YZdk9wD0nV7VXLwii0walkk+8FxGRSLLutI16ltSPLyJSQaICf3yvXlfqiIhMlajAD8e7dLSHLyIyRbIC37SHLyJSTaIC/9hJ2wZXRETkDJSowA/jC3N00lZEZKpkBb5O2oqIVJWowA900lZEpKpEBX4qDnzdbSsiMlWiAj+wY3faiojI8RIV+OrDFxGpLpmBrz58EZEpEhX46tIREakuUYGvPXwRkeoSFfiBhlYQEakqUYEfamgFEZGqEhb40VRdOiIiUyUq8NWlIyJSXaICX+Phi4hUN23gm9kdZrbPzLZXWf5+M3s8fj1kZheXLdttZk+Y2WNm1lPPilcyHviFogJfRGSyWvbw7wQ2nGD588AV7n4R8FngtknLr3L3S9y9++SqWLvxB6BoD19EZKrUdAXc/QEzW3mC5Q+VfXwYWFaHep0UDa0gIlJdvfvwPwzcW/bZgfvNbKuZbTzRima20cx6zKynv7//pH480I1XIiJVTbuHXyszu4oo8N9WNvsyd+8zs4XAd8zsp+7+QKX13f024u6g7u7uk0rsUEMriIhUVZc9fDO7CLgduN7dD4zPd/e+eLoPuBtYX4/fq0ZdOiIi1Z1y4JvZCuAbwAfc/Zmy+e1mNmv8PXA1UPFKn3oJdNJWRKSqabt0zOwu4Eqg08x6gU8DaQB3vxX4FLAA+HOLArcQX5GzCLg7npcCvubu/zQD2zDh2B7+TP6KiMjZqZardG6aZvlHgI9UmL8LuHjqGjNHQyuIiFSXqDttNR6+iEh1iQr8UA8xFxGpKpGBrz18EZGpEhn46sMXEZkqWYGv4ZFFRKpKVOAHGh5ZRKSqRAW+9vBFRKpLVOAHGlpBRKSqRAW+nnglIlJdsgLfNLSCiEg1iQr8IN4a7eGLiEyVqMDXSVsRkeqSFfi7f8B5tkdDK4iIVJCowLdNv8B/CL+voRVERCpIVOATpslaUUMriIhUkKzAD9JkrKA9fBGRCpIV+GGGtBV10lZEpIKEBX6aDOrSERGpZNrAN7M7zGyfmVV8ALlFvmBmO83scTNbV7Zsg5ntiJfdUs+KVxRm1KUjIlJFLXv4dwIbTrD8GmBN/NoIfBnAzELgS/HytcBNZrb2VCo7rTBNhoL28EVEKpg28N39AeDgCYpcD/y1Rx4G5prZYmA9sNPdd7l7DtgUl505YTruw5/RXxEROSvVow9/KfBS2efeeF61+RWZ2UYz6zGznv7+/pOrSZghjbp0REQqqUfgW4V5foL5Fbn7be7e7e7dXV1dJ1eTMENaJ21FRCpK1eE7eoHlZZ+XAX1Apsr8mRNG1+HrskwRkanqsYe/GfhgfLXOpcBhd98LbAHWmNkqM8sAN8ZlZ06QJo0CX0Skkmn38M3sLuBKoNPMeoFPA2kAd78VuAe4FtgJDAMfipcVzOxm4D4gBO5w9ydnYBuOCTOk1KUjIlLRtIHv7jdNs9yBj1dZdg9Rg3B6hGmdtBURqSJhd9pm1KUjIlJFwgI/TcoLeuKViEgFyQt87eGLiFSUsMDPRIGvvBcRmSJxga+TtiIilSUr8IMUKc+rS0dEpIJkBX6YIaRIUaOniYhMkbjAD3Dci42uiYjIGSdhgZ8GwEq5BldEROTMk7DAzwBgRQW+iMhkCQv8aA8/l1Pgi4hMlsjAL+bHGlwREZEzT8ICP+rSKSjwRUSmSGTgF/Pq0hERmSxZgR9Eoz17UTdfiYhMlqzAj/fw0xQYzhUaXBkRkTNLIgM/Q4GRnG6+EhEpl7DAj67SifbwFfgiIuUSFvhxl44VGMkr8EVEytUU+Ga2wcx2mNlOM7ulwvLfNrPH4td2Myua2fx42W4zeyJe1lPvDTjOxB5+UXv4IiKTTPsQczMLgS8B7wR6gS1mttndnxov4+6fBz4fl78O+E13P1j2NVe5+/661rySsi4d9eGLiByvlj389cBOd9/l7jlgE3D9CcrfBNxVj8q9arpKR0SkqloCfynwUtnn3njeFGbWBmwAvl4224H7zWyrmW2s9iNmttHMesysp7+/v4ZqVVAW+OrDFxE5Xi2BbxXmVbur6TrgwUndOZe5+zrgGuDjZnZ5pRXd/TZ373b37q6urhqqVUF841Xa1IcvIjJZLYHfCywv+7wM6KtS9kYmdee4e1883QfcTdRFNDOO69JR4IuIlKsl8LcAa8xslZlliEJ98+RCZjYHuAL4Vtm8djObNf4euBrYXo+KV1QW+KPq0hEROc60V+m4e8HMbgbuA0LgDnd/0sw+Fi+/NS76XuB+dx8qW30RcLeZjf/W19z9n+q5AceJr9LJWlEnbUVEJpk28AHc/R7gnknzbp30+U7gzknzdgEXn1INX4048NvCEoPq0hEROU4i77RtDUu6Dl9EZJJkBX4QggW0hbpKR0RksmQFPkCYoSUoKfBFRCZJaOAXdZWOiMgkyQv8IBXv4esqHRGRcjVdpXNWCTO06E5bEZEpkreHH2bIajx8EZEpEhj4abI6aSsiMkUiA1/PtBURmSqRgZ+2aDx892qDeoqINJ8EBn6GDEVKDmOFUqNrIyJyxkhk4LcEUXfOvsGxBldGROTMkcDAT9MSRnv2ewZGGlwZEZEzRwIDP0OLRTdd9SnwRUQmJC/ws7NJ548ACnwRkXLJC/z2LoLhfjo7surSEREpk8jAZ/Qw584JFfgiImUSGPidAKyZNaYuHRGRMjUFvpltMLMdZrbTzG6psPxKMztsZo/Fr0/Vum7dxYF/XtsIfQOjuvlKRCQ27WiZZhYCXwLeCfQCW8xss7s/NanoD9393Se5bv20dwGwPDvMSD7DwHCeee2ZGfs5EZGzRS17+OuBne6+y91zwCbg+hq//1TWPTlx4C9OHQV0Lb6IyLhaAn8p8FLZ59543mRvNbNtZnavmV3wKtetn7hLZ2EQXZq5a//QjP6ciMjZopbAtwrzJneMPwqc6+4XA38GfPNVrBsVNNtoZj1m1tPf319DtarIzoYww8JwkJZ0wE9ePHTy3yUikiC1BH4vsLzs8zKgr7yAuw+6+9H4/T1A2sw6a1m37Dtuc/dud+/u6up6FZswiRm0dRIOH+DiZXN59AUFvogI1Bb4W4A1ZrbKzDLAjcDm8gJmdo6ZWfx+ffy9B2pZd0a0d8JQP+vOnceTfYN6oLmICDUEvrsXgJuB+4Cngb939yfN7GNm9rG42A3AdjPbBnwBuNEjFdediQ05TnsXDPXzphXzKJScx3sPz/hPioic6Wp6iHncTXPPpHm3lr3/IvDFWtedce1dcOBZ3rhiLgA9Lxxk/ar5p7UKIiJnmuTdaQtxl85+FnRkef3i2Xzv6X2NrpGISMMlM/A7FkF+GIYPsuGCc9j64iH2DY42ulYiIg2VzMBffFE07fsJ11x4Du5w/1OvNLZOIiINltDAvySa9j3KmoUdrO5s59uP721olUREGi2Zgd86Fxa8Bvb8BDPj365byr/uOsCzrxxpdM1ERBommYEPsPRNsGcrADetX0EmFfCVh3Y3tk4iIg2U3MBfsg6OvgyDfSzoyPKeS5bwjUd72XtYg6mJSHNKbuAve3M03f0gAL/6jjW4w+//v5kbmVlE5EyW3MBf8kboOAeejkZyWD6/jV99x2u4d/vLfFdX7IhIE0pu4AcBvP46ePY7kIuGSN54+Xmcf84sfvfuJzg0lGtwBUVETq/kBj7ABe+Bwgg8cx8AmVTAH/37izk0lOPTm2d+SB8RkTNJsgN/xVth9lJ45C8hfrbtBUvm8Gs/u4bN2/q45wldmy8izSPZgR+EcNlvwIsPwfMPTMz+lSvP46Jlc/jEP2xjq8bLF5EmkezAB1j3QZi1GL77GShF4+Knw4DbP9jNwllZfvkrj7B9j4ZPFpHkS37gp1vgnZ+FvkfhoS9MzF44u4W/++ilzG5J84G/+jHP9R9tYCVFRGZe8gMf4MIbYO174Hufg+d/ODF76dxWvvbRtxCY8ZGv9nB4ON+4OoqIzLDmCHwzuO5PYf5q+D/vhwPPTSw6d0E7t37gTfQeGubn/uyHPLzrQAMrKiIyc5oj8CEaUO39/xC9v/tjUCxMLHrzyvnc9dFLSYcBv3j7j/n61t7G1FFEZAY1T+ADzDsXrv0j6H0E7v3t40K/e+V8vnXzZaxfNZ///A/b+OQ3t5MrlBpYWRGR+qop8M1sg5ntMLOdZnZLheXvN7PH49dDZnZx2bLdZvaEmT1mZj31rPxJufAG+Jlfg547YNMvQG54YtHsljR3fmg9H337Kv7m4Rf4nf+7jVLJG1hZEZH6mfYh5mYWAl8C3gn0AlvMbLO7l49C9jxwhbsfMrNrgNuAt5Qtv8rd99ex3ifPDK7+bLS3/+1PwN++D35hE7TMAaK7cX/v59YypzXNH97/DAMjeT717rWs7upocMVFRE5NLXv464Gd7r7L3XPAJuD68gLu/pC7j9/B9DCwrL7VnAFv/gi87/aoe+er18GRl49b/PGrXsMn372WrbsP8fNffJB/2aEHoYvI2a2WwF8KvFT2uTeeV82HgXvLPjtwv5ltNbON1VYys41m1mNmPf39/TVUqw4uvAFuvAv2Pwt/cQW88FB5ffjw21Zx/29dzrJ5rXzoK1t435cfYttLA6enbiIidVZL4FuFeRU7ts3sKqLA/y9lsy9z93XANcDHzezySuu6+23u3u3u3V1dXTVUq05eezV85LuQaYM7fy66Vr/sZO7iOa18/Vd+hk++ey29h4Z5758/yGc2P8nAsEbbFJGzSy2B3wssL/u8DOibXMjMLgJuB65394mL2d29L57uA+4m6iI6syy6AP7TA3DxTfDAH8Ad74L+ZyYWt2dT0d7+b17BjetX8Nf/upsrPv99/upHz+tKHhE5a9QS+FuANWa2yswywI3A5vICZrYC+AbwAXd/pmx+u5nNGn8PXA1sr1fl6yo7C97z53DDHXDwObj1bfDgFybG3wGY05rmv7/3Qu759bdz0bI5fPYfn+LqP/4B9z35Mu66mkdEzmxWS1CZ2bXAnwAhcIe7f87MPgbg7rea2e3A+4AX4lUK7t5tZquJ9uohuiLoa+7+uel+r7u723t6GngF55FX4Nu/BT/9R1j0BnjX52D1lccVcXe+v6Of//btp3iuf4hLV8/no29fzSXL57KgI9uYeotI0zKzre7efcIyZ+KeacMDH6Lx85/6FnznkzDwIrx2QzQIW9drjyuWL5bY9MiL/PF3n+XgUI6WdMCnr7uAn794Ce3Zaa96FRGpCwV+PeRH4ZG/gAf+MHpUYvd/hMs/AbPOOa7YcK7A9j2D/Ok/P8ODOw9gBj97/kJueNNy3rhiLotmtzRoA0SkGSjw62loP3z/f0DPV6Kbt95wA1zxO7DgvOOKFUvOD57ZxyPPH2LTlhcZGM6TCoyb1q/gFy89l9edM6tBGyAiSabAnwkHnoMtt0fBX8xFV/b8zK/CwvOnFB3NF3l67yBff7SXTY+8RKHknH/OLK67eAlXvW4hq7vaaUmHDdgIEUkaBf5MOvIKPPgn0Zg8hVFYfVV09+6aqyGVmVJ8/9Exvv34Xr752B5+8uIAAJkw4J1rF/Hvupfx9jVdhEGlWx5ERKanwD8dhg7Ao3fCI7fDkT5onQ9veF+05790XdT9M0nfwAhbdh/kJy8O8K3H9nBoOE9bJuSiZXO46nULWXfuPNYunq2TviJSMwX+6VQswK5/gW13wU+/He31L3hN1Nf/ug2w+JKK4T9WKPK9p/fx4+cP8vCuA/z05SNAVPS8rg4uWT6Xy1/bxbsuWEQ2pe4fEalMgd8oo4fhqc2wbRO8+BB4KXqQ+ms3wOuugZVvj4ZyqGDf4ChP7DnME3sOs33PYXpeOMTAcJ6Fs7IsnJ1lQXuWt6yez4YLzmFVZztWoRERkeajwD8TDO2HZ++HHffCc9+D3FEIUtEe/4pLYcVbo2l7Z8XViyXnRzv387cPv0ChWKJvYJQdr0RHAR3ZFK9Z2MHy+W10dmRYOreV97xxKZ268Uuk6SjwzzSFMdj9Q9j9I3jxYdizNbrSB2DBmuMbgPmrK3YBAewZGOH7O/bxzMtH2PHKEfYeHuXA0RxHxwqkQ6M1HbJwdgsXLp3DwtlZFs1qYf2q+VywZLaOCEQSSoF/psuPwt7HovB/8WF48V9hdCBa1t4FXefD3HPhNe+ARRfC/FUQpqt+3XP9R/n7LS8xVijx4sFhdrx8hP4jY+SK0QBv2VTA0nmtLJvXxrJ5rSyd28r558zi/MWzWdCe0SWiImcxBf7ZplSC/c9Ewf/iwzDwAux7+lgjEKRhzrLoaV1L1sGybljaDbMWVf1Kd6f/yBg/eKafZ145wp6BEXoPjbDn0AgHho4f4rktEzKvLeoamt+eIQjgitd20ZZJMaslNdFQqGEQOfMo8JOgWICXH4f+HbB/Bwy8FDUK+56CUjxuf6oFOhbCsjdHRwRzlsLs+DV3ObTOq/jVQ2MFtu85zK79QxwaznHwaI6DQzleOjTMwHCeo2MF9h4ePW6dwOB158xmeRz8remQJXNbWTK3haVzW5nXnmHhrKwGkBM5zRT4SZYfgb3bovMAR16Ojgb6fgKDfccagnHj3UPzVsYNwZLjG4WW2RV/wt3Zue8oJYcjo3l6D42wc99RHt9zmFcOj5IrljgyWmD/0bEp63Z2ZJnfnmZOa5rZLfE0fs1pTbOgPcOslhStmZALFs9hdmtK5xdEToECvxmVijDUD4f3wGBvNNJn/w7o/2n0/ug+pjywLDMrbgCWxK9lEKaiweLOfRssWA1tndEzA6rcS/Dy4VH2DIxwePhYw3B4JD/xGhyNpkdGC1PWH5cOjbZMisVzWnh9fF5h/9Ex8iVnyZwW2rMp1i6eTeesLIEZLemAFfPbaMukJp5HoEZDmpUCX6Yq5ODI3uhIYHBP9Dq859j7wb5jjUKQOv5oIcxEwd++ANoWxO87y+aVfW6ZHV2B1LYAMu0TX1EsOUdG8+w/mmNorMDhkTxP7x1kOFckHx8x7BkY4ad7BxkYyTOvLUMqNF4+PMpYlaeLpUOjWHLaMimWzWulLRNOdDe1ZEJaUiGtmYDWeF42nrZm4jLpgAUdWTo7shRLrvMUclaqJfB1736zSWWik77zzq1eppA7FvQv/ThqIIb2w/B+GD4QDScxvB8OvRB9Hhs88W9mZ0c3nrXNJ8SYO38Vc1MtUYMyZymXz18KLXMh2wGZDsguhOx50fuycYlG80We7BvkyGgedzgyVqD30DBHRguEZhwdixqL0XyRkVyRwdE8I7kio/lSNC9+TbePYxbd41AqOSWHpfNaGSsUWTKnldVdHWRTAQPDuahrqiPLgo4MLamQoVyBbCqYaGyONSjR+7ayz5kwINDYSXKaKfBlqlQGiIP2vKumL18YixuCuFEYihuBMB3NH9wbNRojh6K7jnd+N2pQivnpG4swGzUMFtAyewlvmrMMWudCuhXSbdG0JZ7Oa4Pl8fz2ruiIJBUflbTNh1QrbsZYocRYvjTRAIzkivQfHePg0BiGsfvAEAPDeQIzHKdvYIRsKuSFg8N856mXGcuXmNueZnAkOkI56f/MgZFNBcxqSbOgI8PQWIEwiLq1WssaiVQYEBqkw4D2bIqObHTuY7xxaYmPUrKpaNqSjpYFZoRB9EoFRmt8tDOxXA1O01Hgy6lLZY/1/79ao4ejBmFsEMaORK/cURg7Crn4c6kYNRCDe+Bwb3SCOj8C+eFoWhid/ndiFqRoCbO0pDLMCbNR3dsWRI1KcQw6zokuc021ggUQhLCsPWpEzk1F5zHa5gM5yLSTT81lsJBitBjQ2pIl78ZwOIvhQsBYPs/RYoaRIhNHHeONTK5QmngNjOQ5cHSMjq4OiqUSI7kiw7kih4Zz9A0UKZScYsnJFUoMjRUYyhUo1aEnNpMKJrq0WtIhR0cLlNxZ0JHlyGietkyK2a1pWuLGY2VnO+AcHYue8zy7JVoexudNPD43ZBhz29K0pENSgRHEDU4YGC3pkPZMikwqYDRfJBUa2VR0xFNy5+hYga5ZWTJhwFihRKFUOnbElA7JpIKJBiwMTOdsXiUFvjRWy5zodSpKJSiMHGsExo5GJ65LhfjoYz8MH4zeF8fiaS6aFkbjssWoHkf2Rlc7Fceix1wW89F3V5EGFkxXvzATNRiZ9vjIpDW6pyJMR8vCDLS0REcjYRY6yuaHqWg6Xj7VgocZChaS95BC0cmXjNHMXMY8Ra5k5IrOWNEoYRSCLLmghTHLMFIMGS4GDBfiadHi7q4So4UibZkUZnBoKMeslhQj+RIDwzlyhWj5fU++TBgYHfEortFJ+PxxjY8BJfe6NEi1KD+CyaQCFs1qIQyMkjup0AjjI7rhXBEzaE1HR01tmdTEEVQ2FeAeXcoQTR0jakgGR/N0dmTp6sgQBNH3BfFvBsZxR1GBWfz52PxgvHzZ/IllE993rHw2FXDRsrkz9t+rpsA3sw3AnxI9xPx2d/+fk5ZbvPxaYBj4ZXd/tJZ1RU5ZEERhWnZyuK5Kpagh8WJ0RDIyEM3PDUVHJoXRqHEpFaOGZORQ9N6CsiOR+JWLj0pKBSjlowZldCA6b1KMG6BiPvqeYiGe5qLfjhlRQ1P9nutXIUgda0yCVHwnt0WN3HhDlW6Nli1MxeVDsDCajq8zPj9I4UGavAcUCShZipKFuIUUCSkQkPOQgoekUimKFpL3gLxH35nOZBgcc0oWEKbSmBn5EuQKzljRKZSg4EbJiX/DyHtI0Y2xIuwfKlIyAwspuJEvGalsiszcFCUChgswkofh4RJDh52BAowWHLcAJ8AtpERcd4eO1gzbdo0xMFqkhOHYxBTqf3TR2ZGl57/+m7p/77hpA9/MQuBLwDuBXmCLmW1296fKil0DrIlfbwG+DLylxnVFzmxBEJ1QhugoYO6K01+HUikO/7FjJ9VLheiciBejI5hiLvpcKh6bFkaPNTql/PGNSDEfzSuMxd1mcQPkpSjoi7lovcJo3JjF5Se+v3CsgfJi9N2lAlYqkBnvhpt4xd9fXu+zwfjN6BUeSe1YdIbfgonXxDzizxZEZSctd4599ri8m5FvWQA0MPCB9cBOd98FYGabgOuB8tC+Hvhrj67xfNjM5prZYmBlDeuKyHSCAIIWSFdIHogG2zublOKGarwBGG9oJjcS7oDHDVs8Hf9cKsZliscakfHPXjr2G+Xzx79z4n1p0vvy75j0exN1iD7bxLxjZWxSmcnLK31P+fLW7Mw+87qWwF8KvFT2uZdoL366MktrXBcAM9sIbARYsaIBe1AicvoEARCccDBAqb+ghjKVOqomn5KpVqaWdaOZ7re5e7e7d3d1ddVQLREReTVq2cPvBZaXfV4G9NVYJlPDuiIichrUsoe/BVhjZqvMLAPcCGyeVGYz8EGLXAocdve9Na4rIiKnwbR7+O5eMLObgfuILq28w92fNLOPxctvBe4huiRzJ9FlmR860bozsiUiInJCGjxNRCQBahk8rZYuHRERSQAFvohIk1Dgi4g0iTOyD9/M+oEXTnL1TmB/HatztmjW7Ybm3XZtd/M50baf6+4nvInpjAz8U2FmPdOduEiiZt1uaN5t13Y3n1PddnXpiIg0CQW+iEiTSGLg39boCjRIs243NO+2a7ubzylte+L68EVEpLIk7uGLiEgFCnwRkSaRmMA3sw1mtsPMdprZLY2uz0wzs91m9oSZPWZmPfG8+Wb2HTN7Np7Oa3Q9T5WZ3WFm+8xse9m8qttpZr8b/xvYYWbvakytT12V7f6Mme2J/+aPmdm1ZcuSst3LzexfzOxpM3vSzH49nt8Mf/Nq216/v7u7n/UvopE4nwNWE43Bvw1Y2+h6zfA27wY6J837A+CW+P0twP9qdD3rsJ2XA+uA7dNtJ7A2/ttngVXxv4mw0dtQx+3+DPCJCmWTtN2LgXXx+1nAM/H2NcPfvNq21+3vnpQ9/Inn7rp7Dhh/dm6zuR74avz+q8B7GleV+nD3B4CDk2ZX287rgU3uPubuzxMN173+dNSz3qpsdzVJ2u697v5o/P4I8DTRo1Kb4W9ebduredXbnpTAr/ZM3SRz4H4z2xo/DxhgkUcPniGeLmxY7WZWte1shn8HN5vZ43GXz3i3RiK328xWAm8EfkyT/c0nbTvU6e+elMCv+dm5CXKZu68DrgE+bmaXN7pCZ4Ck/zv4MnAecAmwF/ijeH7ittvMOoCvA7/h7oMnKlphXtK2vW5/96QEfi3P3U0Ud++Lp/uAu4kO5V4xs8UA8XRf42o4o6ptZ6L/Hbj7K+5edPcS8JccO3xP1HabWZoo8P7O3b8Rz26Kv3mlba/n3z0pgd9Uz841s3YzmzX+Hrga2E60zb8UF/sl4FuNqeGMq7adm4EbzSxrZquANcAjDajfjBgPvNh7if7mkKDtNjMD/gp42t3/d9mixP/Nq217Xf/ujT4zXccz3NcSndV+Dvi9Rtdnhrd1NdHZ+W3Ak+PbCywA/hl4Np7Ob3Rd67CtdxEdxuaJ9mg+fKLtBH4v/jewA7im0fWv83b/DfAE8Hj8P/viBG7324i6JR4HHotf1zbJ37zattft766hFUREmkRSunRERGQaCnwRkSahwBcRaRIKfBGRJqHAFxFpEgp8EZEmocAXEWkS/x+vQxB8jcypHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "num_epochs =245\n",
    "plt.plot(range(num_epochs),losses_train,range(num_epochs),losses_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
