{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81753e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f9b631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "# TODO: Add necessary transformations\n",
    "# TODO: Load the MNIST dataset\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# TODO: Create data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649f544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.dropout1 = nn.Dropout(p = 0.2)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(p = 0.5)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(-1, 28 * 28)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152d7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.003)\n",
    "losses_train = []\n",
    "losses_valid = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08808d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this code to implement Early stopping\n",
    "patience = 20\n",
    "min_delta = 0.001\n",
    "best_loss = None\n",
    "patience_counter = 0\n",
    "num_epochs=500\n",
    "\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_set, [50000, 10000], generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(val_subset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f9c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.0523513748701703, Validation Loss: 1.5230310191014769\n",
      "Epoch 2, Training Loss: 1.3015491874741594, Validation Loss: 0.8254197148760413\n",
      "Epoch 3, Training Loss: 0.9317041723840018, Validation Loss: 0.6025584302130779\n",
      "Epoch 4, Training Loss: 0.7732582307382941, Validation Loss: 0.5005864370021091\n",
      "Epoch 5, Training Loss: 0.6826043187428131, Validation Loss: 0.442093050688695\n",
      "Epoch 6, Training Loss: 0.6219001025088561, Validation Loss: 0.40211339456260586\n",
      "Epoch 7, Training Loss: 0.5756160114873955, Validation Loss: 0.3732446806066355\n",
      "Epoch 8, Training Loss: 0.541328346837304, Validation Loss: 0.3465114070731363\n",
      "Epoch 9, Training Loss: 0.5171092541487232, Validation Loss: 0.32980433856226077\n",
      "Epoch 10, Training Loss: 0.49494266360680433, Validation Loss: 0.3197869952222344\n",
      "Epoch 11, Training Loss: 0.4722282908745666, Validation Loss: 0.30120235040878796\n",
      "Epoch 12, Training Loss: 0.4518518214509177, Validation Loss: 0.28876002507794435\n",
      "Epoch 13, Training Loss: 0.43796335996341096, Validation Loss: 0.2808703375849754\n",
      "Epoch 14, Training Loss: 0.4242369378807702, Validation Loss: 0.2703821961382392\n",
      "Epoch 15, Training Loss: 0.4112308289387079, Validation Loss: 0.26102612495042715\n",
      "Epoch 16, Training Loss: 0.39692065114977515, Validation Loss: 0.2521240228585377\n",
      "Epoch 17, Training Loss: 0.38264650594133304, Validation Loss: 0.24508207841853427\n",
      "Epoch 18, Training Loss: 0.37779815120102245, Validation Loss: 0.23725294229236377\n",
      "Epoch 19, Training Loss: 0.36540162359981904, Validation Loss: 0.23041118446523975\n",
      "Epoch 20, Training Loss: 0.35711920312218576, Validation Loss: 0.22349762797925124\n",
      "Epoch 21, Training Loss: 0.34657519801593284, Validation Loss: 0.22006811149370897\n",
      "Epoch 22, Training Loss: 0.33678170103730676, Validation Loss: 0.21325726493908342\n",
      "Epoch 23, Training Loss: 0.32875611700578283, Validation Loss: 0.2085083222882763\n",
      "Epoch 24, Training Loss: 0.3268247438328607, Validation Loss: 0.2031883189252987\n",
      "Epoch 25, Training Loss: 0.31527312666273066, Validation Loss: 0.19866381637799513\n",
      "Epoch 26, Training Loss: 0.30991020144970177, Validation Loss: 0.1926450886449237\n",
      "Epoch 27, Training Loss: 0.3046756855079106, Validation Loss: 0.19079491035763624\n",
      "Epoch 28, Training Loss: 0.2999706783179027, Validation Loss: 0.1851963549377812\n",
      "Epoch 29, Training Loss: 0.2873681868984501, Validation Loss: 0.18076968216782163\n",
      "Epoch 30, Training Loss: 0.28638076651166244, Validation Loss: 0.17697919262157882\n",
      "Epoch 31, Training Loss: 0.28260701079803235, Validation Loss: 0.17372175937245607\n",
      "Epoch 32, Training Loss: 0.27672292547089966, Validation Loss: 0.17009876526085435\n",
      "Epoch 33, Training Loss: 0.27199676752821217, Validation Loss: 0.16640462292132863\n",
      "Epoch 34, Training Loss: 0.26802811196554444, Validation Loss: 0.16389299312214942\n",
      "Epoch 35, Training Loss: 0.26604882101101407, Validation Loss: 0.1607642952518858\n",
      "Epoch 36, Training Loss: 0.25905557142804936, Validation Loss: 0.15748380831662256\n",
      "Epoch 37, Training Loss: 0.25323507834726305, Validation Loss: 0.15563647574774778\n",
      "Epoch 38, Training Loss: 0.251128668227652, Validation Loss: 0.15305600013987275\n",
      "Epoch 39, Training Loss: 0.24808318331551704, Validation Loss: 0.14936585813333655\n",
      "Epoch 40, Training Loss: 0.24674115554769155, Validation Loss: 0.146109757636478\n",
      "Epoch 41, Training Loss: 0.2418459657508173, Validation Loss: 0.14490785360763406\n",
      "Epoch 42, Training Loss: 0.23733118866711284, Validation Loss: 0.14230864617237998\n",
      "Epoch 43, Training Loss: 0.23496517490174598, Validation Loss: 0.14130486943017526\n",
      "Epoch 44, Training Loss: 0.2320214685664248, Validation Loss: 0.13959023998279102\n",
      "Epoch 45, Training Loss: 0.22950152533331405, Validation Loss: 0.1367110384355305\n",
      "Epoch 46, Training Loss: 0.22767012242648774, Validation Loss: 0.1333466957495281\n",
      "Epoch 47, Training Loss: 0.22382045238177534, Validation Loss: 0.13003863565101745\n",
      "Epoch 48, Training Loss: 0.22050818093200483, Validation Loss: 0.12877134800811482\n",
      "Epoch 49, Training Loss: 0.21727487091411915, Validation Loss: 0.12691519755846375\n",
      "Epoch 50, Training Loss: 0.21233486943741217, Validation Loss: 0.1246795062402821\n",
      "Epoch 51, Training Loss: 0.21176748223571, Validation Loss: 0.12199215762981563\n",
      "Epoch 52, Training Loss: 0.2093368941830642, Validation Loss: 0.12228873734522587\n",
      "Epoch 53, Training Loss: 0.2077664510547511, Validation Loss: 0.11845137678371494\n",
      "Epoch 54, Training Loss: 0.20620293596358313, Validation Loss: 0.11936955481388006\n",
      "Epoch 55, Training Loss: 0.20463275754931512, Validation Loss: 0.11516541980539158\n",
      "Epoch 56, Training Loss: 0.2023227496791496, Validation Loss: 0.11591735533824202\n",
      "Epoch 57, Training Loss: 0.19724902299357883, Validation Loss: 0.11363831908103957\n",
      "Epoch 58, Training Loss: 0.19663648754517152, Validation Loss: 0.11237358922364225\n",
      "Epoch 59, Training Loss: 0.19456650482327825, Validation Loss: 0.10962584792120252\n",
      "Epoch 60, Training Loss: 0.19473081962592692, Validation Loss: 0.10967723534080633\n",
      "Epoch 61, Training Loss: 0.19027233343007469, Validation Loss: 0.10609831675815924\n",
      "Epoch 62, Training Loss: 0.188175397347222, Validation Loss: 0.1050472277376777\n",
      "Epoch 63, Training Loss: 0.18719262005423687, Validation Loss: 0.10513987144823098\n",
      "Epoch 64, Training Loss: 0.1853123298053866, Validation Loss: 0.10342778337253317\n",
      "Epoch 65, Training Loss: 0.18463218175589657, Validation Loss: 0.10170953721994427\n",
      "Epoch 66, Training Loss: 0.1793113563662526, Validation Loss: 0.09976567691014071\n",
      "Epoch 67, Training Loss: 0.18256242105415635, Validation Loss: 0.09857043769258032\n",
      "Epoch 68, Training Loss: 0.17774037467534226, Validation Loss: 0.09738787053046143\n",
      "Epoch 69, Training Loss: 0.178755339656049, Validation Loss: 0.09681218579577602\n",
      "Epoch 70, Training Loss: 0.17518109091515863, Validation Loss: 0.09689091927828683\n",
      "Epoch 71, Training Loss: 0.17496082047298392, Validation Loss: 0.0940354439922294\n",
      "Epoch 72, Training Loss: 0.17255567888747145, Validation Loss: 0.09362083513016818\n",
      "Epoch 73, Training Loss: 0.16890276154713718, Validation Loss: 0.09184341695883376\n",
      "Epoch 74, Training Loss: 0.1693434449079544, Validation Loss: 0.0921614930496379\n",
      "Epoch 75, Training Loss: 0.16627581852839698, Validation Loss: 0.09002071410524333\n",
      "Epoch 76, Training Loss: 0.16742497387685693, Validation Loss: 0.0890199112272851\n",
      "Epoch 77, Training Loss: 0.1632756469711693, Validation Loss: 0.08808825673108363\n",
      "Epoch 78, Training Loss: 0.16619860108242804, Validation Loss: 0.08644381748915762\n",
      "Epoch 79, Training Loss: 0.16403151396066268, Validation Loss: 0.08615266460818564\n",
      "Epoch 80, Training Loss: 0.16067481933554797, Validation Loss: 0.08638703890121097\n",
      "Epoch 81, Training Loss: 0.15914214341474303, Validation Loss: 0.08545170999339717\n",
      "Epoch 82, Training Loss: 0.15966154891711626, Validation Loss: 0.08344494427145002\n",
      "Epoch 83, Training Loss: 0.1579034912572709, Validation Loss: 0.08187623252309147\n",
      "Epoch 84, Training Loss: 0.15613934209645747, Validation Loss: 0.08289130265782972\n",
      "Epoch 85, Training Loss: 0.1540942820909022, Validation Loss: 0.0800585125211128\n",
      "Epoch 86, Training Loss: 0.15022261501915418, Validation Loss: 0.0797130848654801\n",
      "Epoch 87, Training Loss: 0.15456951639331035, Validation Loss: 0.07887249062383536\n",
      "Epoch 88, Training Loss: 0.15016437956352413, Validation Loss: 0.07714209143535063\n",
      "Epoch 89, Training Loss: 0.15330779885074922, Validation Loss: 0.07685929564106617\n",
      "Epoch 90, Training Loss: 0.14973676665179703, Validation Loss: 0.07646021336483158\n",
      "Epoch 91, Training Loss: 0.14936618283311567, Validation Loss: 0.07579066428492308\n",
      "Epoch 92, Training Loss: 0.1484549234110886, Validation Loss: 0.07392032947806511\n",
      "Epoch 93, Training Loss: 0.1452446039944792, Validation Loss: 0.07335658058832595\n",
      "Epoch 94, Training Loss: 0.14666763175803144, Validation Loss: 0.07333643906482845\n",
      "Epoch 95, Training Loss: 0.1456675733517069, Validation Loss: 0.07330107947835213\n",
      "Epoch 96, Training Loss: 0.14162021291170165, Validation Loss: 0.07329417326173206\n",
      "Epoch 97, Training Loss: 0.14407508017054435, Validation Loss: 0.06993544108388577\n",
      "Epoch 98, Training Loss: 0.14167584714962284, Validation Loss: 0.06963484745567582\n",
      "Epoch 99, Training Loss: 0.1377865006344945, Validation Loss: 0.06911885221696394\n",
      "Epoch 100, Training Loss: 0.13844775951215263, Validation Loss: 0.06802902607377737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, Training Loss: 0.13852878608135208, Validation Loss: 0.06786038523694132\n",
      "Epoch 102, Training Loss: 0.1371844060151085, Validation Loss: 0.06836266949873679\n",
      "Epoch 103, Training Loss: 0.13904214897024225, Validation Loss: 0.06657004050145246\n",
      "Epoch 104, Training Loss: 0.13699011132518238, Validation Loss: 0.06654847453787306\n",
      "Epoch 105, Training Loss: 0.13421431095448574, Validation Loss: 0.06559496359591177\n",
      "Epoch 106, Training Loss: 0.12967321536998186, Validation Loss: 0.06549865057822435\n",
      "Epoch 107, Training Loss: 0.136231146638653, Validation Loss: 0.06551941877800473\n",
      "Epoch 108, Training Loss: 0.13396959516332982, Validation Loss: 0.06465518615463642\n",
      "Epoch 109, Training Loss: 0.13242400579972627, Validation Loss: 0.06359690712664015\n",
      "Epoch 110, Training Loss: 0.1320389906015954, Validation Loss: 0.06321755709174047\n",
      "Epoch 111, Training Loss: 0.13170754895019315, Validation Loss: 0.06372152668999354\n",
      "Epoch 112, Training Loss: 0.12973940548604168, Validation Loss: 0.061003345846358664\n",
      "Epoch 113, Training Loss: 0.12850346292521972, Validation Loss: 0.060566496621568204\n",
      "Epoch 114, Training Loss: 0.12604748765165522, Validation Loss: 0.060121047528531806\n",
      "Epoch 115, Training Loss: 0.1273475169960751, Validation Loss: 0.06026052784732288\n",
      "Epoch 116, Training Loss: 0.12484035412393717, Validation Loss: 0.05910127337838102\n",
      "Epoch 117, Training Loss: 0.12515405744417454, Validation Loss: 0.058866675771108476\n",
      "Epoch 118, Training Loss: 0.12442037558951564, Validation Loss: 0.05825074786130505\n",
      "Epoch 119, Training Loss: 0.12256234075063899, Validation Loss: 0.05806685484661039\n",
      "Epoch 120, Training Loss: 0.12215271411237241, Validation Loss: 0.057968437653865404\n",
      "Epoch 121, Training Loss: 0.12348949506994027, Validation Loss: 0.05782717271785068\n",
      "Epoch 122, Training Loss: 0.12212129076148497, Validation Loss: 0.05635323008124948\n",
      "Epoch 123, Training Loss: 0.11983195678599036, Validation Loss: 0.05676866481288271\n",
      "Epoch 124, Training Loss: 0.11858330805625107, Validation Loss: 0.05507185447302641\n",
      "Epoch 125, Training Loss: 0.1214426229320673, Validation Loss: 0.05566303817863772\n",
      "Epoch 126, Training Loss: 0.12196710168111966, Validation Loss: 0.054678749396898756\n",
      "Epoch 127, Training Loss: 0.11814825205322999, Validation Loss: 0.053155762755911155\n",
      "Epoch 128, Training Loss: 0.11929554121294764, Validation Loss: 0.05384871798454766\n",
      "Epoch 129, Training Loss: 0.1186294306967177, Validation Loss: 0.05453161289618843\n",
      "Epoch 130, Training Loss: 0.11480666627622903, Validation Loss: 0.05271725946515324\n",
      "Epoch 131, Training Loss: 0.1171970085989533, Validation Loss: 0.051637032107823784\n",
      "Epoch 132, Training Loss: 0.11605324447780514, Validation Loss: 0.05099936337714457\n",
      "Epoch 133, Training Loss: 0.1145539786755991, Validation Loss: 0.0514259765882662\n",
      "Epoch 134, Training Loss: 0.11422273744620494, Validation Loss: 0.051480278068008554\n",
      "Epoch 135, Training Loss: 0.11464310207191719, Validation Loss: 0.05072987946135935\n",
      "Epoch 136, Training Loss: 0.11368011954083626, Validation Loss: 0.04932002453682173\n",
      "Epoch 137, Training Loss: 0.11170260063203763, Validation Loss: 0.050098778620647016\n",
      "Epoch 138, Training Loss: 0.11094347308419629, Validation Loss: 0.049109623037059166\n",
      "Epoch 139, Training Loss: 0.11408221397374167, Validation Loss: 0.04923257101208541\n",
      "Epoch 140, Training Loss: 0.11073550190637584, Validation Loss: 0.04788237936486294\n",
      "Epoch 141, Training Loss: 0.1073507951634096, Validation Loss: 0.048522846457388275\n",
      "Epoch 142, Training Loss: 0.10897489704354517, Validation Loss: 0.04743195417675243\n",
      "Epoch 143, Training Loss: 0.1094161077613023, Validation Loss: 0.04740893852060578\n",
      "Epoch 144, Training Loss: 0.1090833515570219, Validation Loss: 0.046765636564469686\n",
      "Epoch 145, Training Loss: 0.10894446535659497, Validation Loss: 0.046519430680497055\n",
      "Epoch 146, Training Loss: 0.10594791078320476, Validation Loss: 0.04503786543778672\n",
      "Epoch 147, Training Loss: 0.10887296474290523, Validation Loss: 0.04526922427366968\n",
      "Epoch 148, Training Loss: 0.10576952605156788, Validation Loss: 0.04606228090665499\n",
      "Epoch 149, Training Loss: 0.10694238208512317, Validation Loss: 0.044756721871294036\n",
      "Epoch 150, Training Loss: 0.10565902889946471, Validation Loss: 0.04438253646274542\n",
      "Epoch 151, Training Loss: 0.10641884593876885, Validation Loss: 0.043833551771944736\n",
      "Epoch 152, Training Loss: 0.1068343125811931, Validation Loss: 0.04316015593013518\n",
      "Epoch 153, Training Loss: 0.10053654789213719, Validation Loss: 0.04349837096031328\n",
      "Epoch 154, Training Loss: 0.10170474807535217, Validation Loss: 0.04340520998534218\n",
      "Epoch 155, Training Loss: 0.10249909034955984, Validation Loss: 0.042757680535073256\n",
      "Epoch 156, Training Loss: 0.10203147002422353, Validation Loss: 0.04273800783016523\n",
      "Epoch 157, Training Loss: 0.10232237605417747, Validation Loss: 0.04192208725763307\n",
      "Epoch 158, Training Loss: 0.10228458812921, Validation Loss: 0.04178301376219436\n",
      "Epoch 159, Training Loss: 0.10134680468096599, Validation Loss: 0.041970443347569816\n",
      "Epoch 160, Training Loss: 0.10142159739088243, Validation Loss: 0.0406534306713896\n",
      "Epoch 161, Training Loss: 0.09993613018954137, Validation Loss: 0.04102326302409504\n",
      "Epoch 162, Training Loss: 0.10321336478364271, Validation Loss: 0.04071854198926905\n",
      "Epoch 163, Training Loss: 0.09807218355077035, Validation Loss: 0.04000419673693788\n",
      "Epoch 164, Training Loss: 0.09829898142138682, Validation Loss: 0.039884901337123885\n",
      "Epoch 165, Training Loss: 0.0979494861692492, Validation Loss: 0.038913071848108036\n",
      "Epoch 166, Training Loss: 0.0991245263845904, Validation Loss: 0.038759963562457\n",
      "Epoch 167, Training Loss: 0.09836572972731962, Validation Loss: 0.04003820408011081\n",
      "Epoch 168, Training Loss: 0.09528341744222175, Validation Loss: 0.03786685378335203\n",
      "Epoch 169, Training Loss: 0.09621723262823499, Validation Loss: 0.0391632786367658\n",
      "Epoch 170, Training Loss: 0.09617017331257113, Validation Loss: 0.037690098381120786\n",
      "Epoch 171, Training Loss: 0.0943909276560791, Validation Loss: 0.038441182133092125\n",
      "Epoch 172, Training Loss: 0.09665507693856415, Validation Loss: 0.03696768434872483\n",
      "Epoch 173, Training Loss: 0.09545029162951528, Validation Loss: 0.0373224169920513\n",
      "Epoch 174, Training Loss: 0.09613217691581116, Validation Loss: 0.036341551974546284\n",
      "Epoch 175, Training Loss: 0.09202949949669273, Validation Loss: 0.037141316172898196\n",
      "Epoch 176, Training Loss: 0.09595563002566157, Validation Loss: 0.03589713073388976\n",
      "Epoch 177, Training Loss: 0.09474949194773682, Validation Loss: 0.035706119300800546\n",
      "Epoch 178, Training Loss: 0.09114985096194883, Validation Loss: 0.03548447753281991\n",
      "Epoch 179, Training Loss: 0.09437718153891485, Validation Loss: 0.036578255004077485\n",
      "Epoch 180, Training Loss: 0.09086276730423225, Validation Loss: 0.03577782981732421\n",
      "Epoch 181, Training Loss: 0.09351926970817863, Validation Loss: 0.03572858548441392\n",
      "Epoch 182, Training Loss: 0.09264552351166762, Validation Loss: 0.034239378709577405\n",
      "Epoch 183, Training Loss: 0.09153553531932107, Validation Loss: 0.034927991891404386\n",
      "Epoch 184, Training Loss: 0.0916446360242146, Validation Loss: 0.03425454421029693\n",
      "Epoch 185, Training Loss: 0.09089658304012374, Validation Loss: 0.03380389122112957\n",
      "Epoch 186, Training Loss: 0.09154094882887294, Validation Loss: 0.033606960579076674\n",
      "Epoch 187, Training Loss: 0.08900590753480633, Validation Loss: 0.03366194796899725\n",
      "Epoch 188, Training Loss: 0.09000270985074833, Validation Loss: 0.03304196573915494\n",
      "Epoch 189, Training Loss: 0.08772421614137857, Validation Loss: 0.032302476151677285\n",
      "Epoch 190, Training Loss: 0.0898249808277911, Validation Loss: 0.03276942048820958\n",
      "Epoch 191, Training Loss: 0.08924839375858336, Validation Loss: 0.0324050787974232\n",
      "Epoch 192, Training Loss: 0.08820513798110584, Validation Loss: 0.03183895351133861\n",
      "Epoch 193, Training Loss: 0.08829293948814654, Validation Loss: 0.03170003378941755\n",
      "Epoch 194, Training Loss: 0.08759219354407778, Validation Loss: 0.031238990407259127\n",
      "Epoch 195, Training Loss: 0.08744328619211293, Validation Loss: 0.03189490033479728\n",
      "Epoch 196, Training Loss: 0.08479764738153897, Validation Loss: 0.030934782818502562\n",
      "Epoch 197, Training Loss: 0.08733874934612275, Validation Loss: 0.030902162958737343\n",
      "Epoch 198, Training Loss: 0.08391338416925276, Validation Loss: 0.031029347174617042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199, Training Loss: 0.08879730522310905, Validation Loss: 0.03107602456525253\n",
      "Epoch 200, Training Loss: 0.08597540349931891, Validation Loss: 0.030734151332627295\n",
      "Epoch 201, Training Loss: 0.08622030548250942, Validation Loss: 0.02986671410020512\n",
      "Epoch 202, Training Loss: 0.08637463618745046, Validation Loss: 0.029588873275692106\n",
      "Epoch 203, Training Loss: 0.08685855508527791, Validation Loss: 0.030338876908140813\n",
      "Epoch 204, Training Loss: 0.08294089241767488, Validation Loss: 0.028969780921233094\n",
      "Epoch 205, Training Loss: 0.08478338356767255, Validation Loss: 0.02911739752577487\n",
      "Epoch 206, Training Loss: 0.08179342005052355, Validation Loss: 0.028545245159325446\n",
      "Epoch 207, Training Loss: 0.08259868250836108, Validation Loss: 0.028554905870718773\n",
      "Epoch 208, Training Loss: 0.08150129266647196, Validation Loss: 0.02793811524441098\n",
      "Epoch 209, Training Loss: 0.08079720431589273, Validation Loss: 0.02837281546163948\n",
      "Epoch 210, Training Loss: 0.0804916457749649, Validation Loss: 0.027960039246649405\n",
      "Epoch 211, Training Loss: 0.0811766254485273, Validation Loss: 0.028068753369790235\n",
      "Epoch 212, Training Loss: 0.0817020300945549, Validation Loss: 0.02739444704263643\n",
      "Epoch 213, Training Loss: 0.08031248455313143, Validation Loss: 0.027742122487197658\n",
      "Epoch 214, Training Loss: 0.0809232145964877, Validation Loss: 0.026962616350383137\n",
      "Epoch 215, Training Loss: 0.0805695527178575, Validation Loss: 0.02708605234733646\n",
      "Epoch 216, Training Loss: 0.0818272996321519, Validation Loss: 0.02781081805654481\n",
      "Epoch 217, Training Loss: 0.08119707290124871, Validation Loss: 0.02704157080548774\n",
      "Epoch 218, Training Loss: 0.0801580443398467, Validation Loss: 0.02693194342602139\n",
      "Epoch 219, Training Loss: 0.07999257769372099, Validation Loss: 0.026482397752701288\n",
      "Epoch 220, Training Loss: 0.07769730842378396, Validation Loss: 0.026679855833648687\n",
      "Epoch 221, Training Loss: 0.08017921565112465, Validation Loss: 0.027208192891642974\n",
      "Epoch 222, Training Loss: 0.07919148922955463, Validation Loss: 0.026258773158547605\n",
      "Epoch 223, Training Loss: 0.07831485410318223, Validation Loss: 0.025656622350304893\n",
      "Epoch 224, Training Loss: 0.0769478456541173, Validation Loss: 0.026100140126128414\n",
      "Epoch 225, Training Loss: 0.07802756233855701, Validation Loss: 0.0248996268499607\n",
      "Epoch 226, Training Loss: 0.07678949301377304, Validation Loss: 0.02619106752726036\n",
      "Epoch 227, Training Loss: 0.07678954085873277, Validation Loss: 0.024924680747456587\n",
      "Epoch 228, Training Loss: 0.07544406342939702, Validation Loss: 0.02529682705711253\n",
      "Epoch 229, Training Loss: 0.07903813952550308, Validation Loss: 0.025108008416722512\n",
      "Epoch 230, Training Loss: 0.07710499244867199, Validation Loss: 0.025247077240718376\n",
      "Epoch 231, Training Loss: 0.07802339488598727, Validation Loss: 0.02486590025976158\n",
      "Epoch 232, Training Loss: 0.07508887570035626, Validation Loss: 0.02539520869461618\n",
      "Epoch 233, Training Loss: 0.07603890288458355, Validation Loss: 0.024686932789568142\n",
      "Epoch 234, Training Loss: 0.07689627507693574, Validation Loss: 0.023438911161320224\n",
      "Epoch 235, Training Loss: 0.07362185368266727, Validation Loss: 0.02493497695711554\n",
      "Epoch 236, Training Loss: 0.07539162499262199, Validation Loss: 0.02322319750807539\n",
      "Epoch 237, Training Loss: 0.07574009849801104, Validation Loss: 0.0232629777493243\n",
      "Epoch 238, Training Loss: 0.07607385629653009, Validation Loss: 0.02342397410120032\n",
      "Epoch 239, Training Loss: 0.07420372914300481, Validation Loss: 0.0231913715548321\n",
      "Epoch 240, Training Loss: 0.07375641451376096, Validation Loss: 0.02329369668053926\n",
      "Epoch 241, Training Loss: 0.07350635619462728, Validation Loss: 0.022437609254625762\n",
      "Epoch 242, Training Loss: 0.07469923843963266, Validation Loss: 0.023006317112668994\n",
      "Epoch 243, Training Loss: 0.0746396938671491, Validation Loss: 0.023133167366357815\n",
      "Epoch 244, Training Loss: 0.0734834111467409, Validation Loss: 0.021960952956152333\n",
      "Epoch 245, Training Loss: 0.07307877766985152, Validation Loss: 0.022025324700283077\n",
      "Epoch 246, Training Loss: 0.07267112240020528, Validation Loss: 0.0219951148992434\n",
      "Epoch 247, Training Loss: 0.0732424891045464, Validation Loss: 0.02314031194816907\n",
      "Epoch 248, Training Loss: 0.07321382193133108, Validation Loss: 0.023365916364313146\n",
      "Epoch 249, Training Loss: 0.072021569349249, Validation Loss: 0.021310601899530858\n",
      "Epoch 250, Training Loss: 0.07243161782749426, Validation Loss: 0.0217007048840332\n",
      "Epoch 251, Training Loss: 0.07237260911002089, Validation Loss: 0.021076560604006384\n",
      "Epoch 252, Training Loss: 0.07110851963233354, Validation Loss: 0.021429736527190157\n",
      "Epoch 253, Training Loss: 0.07134341413149241, Validation Loss: 0.022201268797648395\n",
      "Epoch 254, Training Loss: 0.07261010041005656, Validation Loss: 0.020615205296632615\n",
      "Epoch 255, Training Loss: 0.0717923057221893, Validation Loss: 0.02171007625737886\n",
      "Epoch 256, Training Loss: 0.06980794186699889, Validation Loss: 0.021615207963474808\n",
      "Epoch 257, Training Loss: 0.07138030971142291, Validation Loss: 0.020379015627898466\n",
      "Epoch 258, Training Loss: 0.06993061786781607, Validation Loss: 0.02051144164718486\n",
      "Epoch 259, Training Loss: 0.06946409840001734, Validation Loss: 0.020216617273696207\n",
      "Epoch 260, Training Loss: 0.06869666277617613, Validation Loss: 0.01948783164740107\n",
      "Epoch 261, Training Loss: 0.07215523998936149, Validation Loss: 0.020923580158506313\n",
      "Epoch 262, Training Loss: 0.06977691325217064, Validation Loss: 0.019881206998382073\n",
      "Epoch 263, Training Loss: 0.06906929491822526, Validation Loss: 0.020107899944499658\n",
      "Epoch 264, Training Loss: 0.07013873826277488, Validation Loss: 0.020800130776942347\n",
      "Epoch 265, Training Loss: 0.06977572322124119, Validation Loss: 0.019355081967382164\n",
      "Epoch 266, Training Loss: 0.06803227452658561, Validation Loss: 0.01999339266201173\n",
      "Epoch 267, Training Loss: 0.06850464695564179, Validation Loss: 0.019930127849075704\n",
      "Epoch 268, Training Loss: 0.06795440451986691, Validation Loss: 0.01957859121341626\n",
      "Epoch 269, Training Loss: 0.06812474726245546, Validation Loss: 0.019791511886799115\n",
      "Epoch 270, Training Loss: 0.06670150277651608, Validation Loss: 0.018935702489592598\n",
      "Epoch 271, Training Loss: 0.06803745651113263, Validation Loss: 0.018935394460953783\n",
      "Epoch 272, Training Loss: 0.06644376987484155, Validation Loss: 0.01870294542301737\n",
      "Epoch 273, Training Loss: 0.06691116344894586, Validation Loss: 0.01840024218252403\n",
      "Epoch 274, Training Loss: 0.06885703597545449, Validation Loss: 0.018120691041939647\n",
      "Epoch 275, Training Loss: 0.0663721750915817, Validation Loss: 0.01855577177710677\n",
      "Epoch 276, Training Loss: 0.06470004690270911, Validation Loss: 0.018793482813356847\n",
      "Epoch 277, Training Loss: 0.06588173300268124, Validation Loss: 0.01864116820714371\n",
      "Epoch 278, Training Loss: 0.06606278385691988, Validation Loss: 0.018658034086181444\n",
      "Epoch 279, Training Loss: 0.06657057810303714, Validation Loss: 0.017783671245460878\n",
      "Epoch 280, Training Loss: 0.06525988853884054, Validation Loss: 0.0181336006393232\n",
      "Epoch 281, Training Loss: 0.06426371221141672, Validation Loss: 0.01825849451814262\n",
      "Epoch 282, Training Loss: 0.0653089081410811, Validation Loss: 0.017819579123448177\n",
      "Epoch 283, Training Loss: 0.06648321711926905, Validation Loss: 0.017233916278965592\n",
      "Epoch 284, Training Loss: 0.06548383046683273, Validation Loss: 0.01814707888087857\n",
      "Epoch 285, Training Loss: 0.06514504840493258, Validation Loss: 0.017565672766160705\n",
      "Epoch 286, Training Loss: 0.0635936549449685, Validation Loss: 0.017094069419304133\n",
      "Epoch 287, Training Loss: 0.06205844691793706, Validation Loss: 0.016698448136293145\n",
      "Epoch 288, Training Loss: 0.0641534640870766, Validation Loss: 0.01691024242210718\n",
      "Epoch 289, Training Loss: 0.06521686510905735, Validation Loss: 0.01733490425958387\n",
      "Epoch 290, Training Loss: 0.06572808457592101, Validation Loss: 0.01704497704175627\n",
      "Epoch 291, Training Loss: 0.062495818252174824, Validation Loss: 0.017547674907540462\n",
      "Epoch 292, Training Loss: 0.06280176980651296, Validation Loss: 0.01579745058163075\n",
      "Epoch 293, Training Loss: 0.0635954036689135, Validation Loss: 0.015929882356802084\n",
      "Epoch 294, Training Loss: 0.06407145291382173, Validation Loss: 0.01648565695549148\n",
      "Epoch 295, Training Loss: 0.06254580096110328, Validation Loss: 0.01647654261531243\n",
      "Epoch 296, Training Loss: 0.0643612481038104, Validation Loss: 0.015845535699149097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297, Training Loss: 0.06216957399706994, Validation Loss: 0.01591071532557238\n",
      "Epoch 298, Training Loss: 0.06250582854481919, Validation Loss: 0.015305448104560696\n",
      "Epoch 299, Training Loss: 0.06373785094155479, Validation Loss: 0.016202621429989555\n",
      "Epoch 300, Training Loss: 0.06248827168379408, Validation Loss: 0.016993281381630344\n",
      "Epoch 301, Training Loss: 0.06191860833319464, Validation Loss: 0.016393713954136156\n",
      "Epoch 302, Training Loss: 0.06225399794414449, Validation Loss: 0.015638595857919983\n",
      "Epoch 303, Training Loss: 0.06272153653947116, Validation Loss: 0.015312251807665038\n",
      "Epoch 304, Training Loss: 0.06113875634483755, Validation Loss: 0.015512017913015892\n",
      "Epoch 305, Training Loss: 0.06188452527272517, Validation Loss: 0.01585080703182122\n",
      "Epoch 306, Training Loss: 0.06061002374517479, Validation Loss: 0.015526902150054266\n",
      "Epoch 307, Training Loss: 0.061375932662493624, Validation Loss: 0.01604773026528089\n",
      "Epoch 308, Training Loss: 0.060921755855156384, Validation Loss: 0.016921045902262257\n",
      "Epoch 309, Training Loss: 0.06125929245381737, Validation Loss: 0.01587312131086483\n",
      "Epoch 310, Training Loss: 0.06318211528077436, Validation Loss: 0.0149387924056604\n",
      "Epoch 311, Training Loss: 0.06017361492661239, Validation Loss: 0.015365681422104656\n",
      "Epoch 312, Training Loss: 0.059717114701041026, Validation Loss: 0.015128457082537815\n",
      "Epoch 313, Training Loss: 0.06140773216444157, Validation Loss: 0.015110516889728191\n",
      "Epoch 314, Training Loss: 0.060256178021184735, Validation Loss: 0.014497458091514351\n",
      "Epoch 315, Training Loss: 0.059932895320733745, Validation Loss: 0.01490717496467861\n",
      "Epoch 316, Training Loss: 0.06201952861037169, Validation Loss: 0.01455805116771664\n",
      "Epoch 317, Training Loss: 0.061067390041664116, Validation Loss: 0.015203418971297396\n",
      "Epoch 318, Training Loss: 0.05911745279575629, Validation Loss: 0.014384316497743367\n",
      "Early stopping triggered! 0.015305448104560696\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping (with one layer model)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # TODO: Training pass\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    # evaluation phase\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    # Calculate average losses\n",
    "    training_loss = running_loss / len(train_loader)\n",
    "    losses_train.append(training_loss)\n",
    "    validation_loss /= len(validation_loader)\n",
    "    losses_valid.append(validation_loss)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {training_loss}, Validation Loss: {validation_loss}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if best_loss is None or validation_loss < best_loss - min_delta:\n",
    "        best_loss = validation_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\", best_loss)\n",
    "            break\n",
    "\n",
    "print(\"Training is finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891e79d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "0.9811\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "with torch.no_grad():\n",
    "  for i in range(10000):\n",
    "    yval = model.forward(test_set[i][0])\n",
    "    if yval.argmax().item() == test_set[i][1] :\n",
    "      correct +=1\n",
    "\n",
    "print(model)\n",
    "print(correct/10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e80afbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x178451c2280>,\n",
       " <matplotlib.lines.Line2D at 0x178451c2340>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_epochs =318\n",
    "plt.plot(range(num_epochs),losses_train,range(num_epochs),losses_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385263ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Cusofayy/MNIST_elementary_neural_network.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551ded1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
